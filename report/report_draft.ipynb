{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Swarm Optimization\n",
    "\n",
    "### Group 3\n",
    "\n",
    "### Scientific Introduction\n",
    "\n",
    "#### Background and Algorithm\n",
    "\n",
    "The particle swarm optimization algorithm was first developed by James Kennedy and Russell Eberhart in 1995, who were inspired by the movement of schools of fish and flocks of birds. Computer simulations of the flocking behavior of animals are common in the field of artificial life and are often used for computer animations (Hu). Kennedy and Russell observed that in a group, these animals were able to utilize both their own knowledge as well as the group's knowledge in order to navigate (1995). The particle swarm optimization method uses this idea by iteratively moving particles across the search space toward a weighted average of their own best position and the group's best position (1995). The algorithm is given below (Poli, Kennedy, and Blackwell):\n",
    "\n",
    "1. Initialize a set of particles with random positions and velocities in the search space\n",
    "2. For each particle:\n",
    "    1. Evaluate the fitness function\n",
    "    2. Compare the current fitness value with the particle's best fitness value ($p_k^i$). If the current value is better, set $p_k^i$ equal to the current value.\n",
    "3. Find the particle with the best fitness value and set $p_k^g$ to be that value.\n",
    "4. For each particle:\n",
    "    1. Update the position and velocity of particle $i$ at iteration $k+1$ according to the equations:\n",
    "    $$x_{k+1}^i = x_k^i + v^i_{k+1} \\tag{1}$$\n",
    "    $$v_{k+1}^i = w_kv_k^i + c_1r_1(p_k^i-x_k^i)+c_2r_2(p_k^g-x_k^i) \\tag{2}$$\n",
    "        where $w_k$ is an inerta parameter, $c_1$ is a cognitive parameter, $c_2$ is a social parameter, and $r_1$ and $r_2$ are random numbers in (0,1)\n",
    "    2. Repeat until convergence\n",
    "\n",
    "The algorithm starts out with particles moving randomly around the search space, then in each iteration, takes note of each particle's best position and the best position in the group, updating the particles' movements to be in the average direction of these two points. The particles' personal best positions and the global best position are weighted by parameters $c_1$ and $c_2$, as well as random components $r_1$ and $r_2$. The general role of these parameters is to create a balance between exploration and exploitation.  \n",
    "\n",
    "\"Exploration\" refers to the ability of the algorithm to test many different parts of the search space, while \"exploitation\" is the precision with which the algorithm is able to concentrate around an optimum (Trelea). A higher level of exploration means the algorithm is less likely to converge to a local optimum rather than a global one, while a higher level of exploitation means that the algorithm will converge more quickly, but it might be to a local optimum rather than a global one. The trade-off between these two features depends largely on the function being optimized.  \n",
    "\n",
    "#### Parameter Description\n",
    "\n",
    "The number of particles used is one important parameter for particle swarm optimization. Typically, 20-50 particles are used, depending on the difficulty of the optimization problem (Poli, Kennedy, and Blackwell). A large number of particles means that the function will need to be evaluated many times during each iteration, which could slow down the algorithm, but using too few particles will cause the algorithm to converge more slowly (Trelea). \n",
    "\n",
    "The inertia parameter, $w_k$, effects how easily the particles will move through the search space. Poli, Kennedy, and Blackwell describe it as the \"fluidity of the medium in which a particle moves\" (2007). $w_k$ typically takes values between 0 and 1, and when it is close to zero, the exploration of the algorithm will be low. On the other hand, when $w_k$ is close to 1, exploration will be high, with particles moving through \"low viscosity medium\" (Poli, Kennedy, and Blackwell). One strategy that is commonly employed to balance these effects is starting the algorithm with a high inertia coefficient and gradually reducing it toward zero as the iterations progress. \n",
    "\n",
    "The parameters $c_1$ and $c_2$ are sometimes called the attraction coefficients, and are used to weight the contributions of a particle's own best position and the global best position. These parameters are usually chosen so that they have an average between 0 and 4 (Trelea). This value comes from rewriting the algorithm equations as follows, ignoring the random components for now:  \n",
    "$$v_{k+1} = wv_k + c(p - x_k) \\tag{3}$$\n",
    "$$x_{k+1} = x_k + v_{k+1} \\tag{4}$$  \n",
    "\n",
    "where $c = \\frac{c_1 + c_2}{2}$ and $p = \\frac{c_1}{c_1 + c_2}p_1 + \\frac{c_2}{c_1 + c_2}p_2$.  \n",
    "\n",
    "Then we can re-write this in matrix form as:  \n",
    "\n",
    "$$y_{k+1} = Wy_k + Cp \\tag{5}$$ with\n",
    "$y_k = \\begin{bmatrix}\n",
    "x_k \\\\\n",
    "y_k\n",
    "\\end{bmatrix}$, $W = \\begin{bmatrix}\n",
    "1-c & w \\\\\n",
    "-c & w\n",
    "\\end{bmatrix}$, $C = \\begin{bmatrix}\n",
    "c \\\\\n",
    "c\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "A particle will be at equilibrium, meaning its position and velocity will no longer change with each iteration, when $y_k = y_{k+1}$, which will happen when $y = \\begin{bmatrix} p & 0 \\end{bmatrix}^T$. \n",
    "\n",
    "The purpose of the random components $r_1$ and $r_2$ is to slow convergence of the algorithm, thereby increasing the level of exploration. They are usually set randomly from a Uniform\\[0,1\\] distribution (Trelea). \n",
    "\n",
    "Trelea suggests that the best method for choosing parameters is to start with those that will allow the algorithm to converge quickly, and if it gives different solutions each time it is run, use parameters that will slow the convergence until the same result can be obtained consistently (2002). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm  \n",
    "Explanation of terms  \n",
    "Exploration/exploitation tradeoff  \n",
    "Parameter choice\n",
    "\n",
    "Convergence\n",
    "Applications\n",
    "\n",
    "Parallelization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
